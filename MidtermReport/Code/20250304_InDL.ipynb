{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow instructions from https://github.com/AierLab/indl-dataset to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Subset \n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "#from scripts import Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/leery/Documents/EllaNB240/indl-dataset-main\n"
     ]
    }
   ],
   "source": [
    "# Set wd\n",
    "\n",
    "path_to_set = \"/Users/leery/Documents/EllaNB240/indl-dataset-main\" # paste path here\n",
    "os.chdir(path_to_set)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate data\n",
    "# !python data/data_generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG #\n",
    "# Adapted from Assignment 2 codebase\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "\n",
    "# LEARNING RATE SETTINGS\n",
    "BASE_LR = 0.001\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "EPOCH_DECAY = 30 # number of epochs after which the learning rate is decayed exponentially by DECAY_WEIGHT.\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 2 # set the number of classes in your dataset\n",
    "DATA_DIR = 'InDL' # to run with the sample dataset, just set to 'hymenoptera_data'\n",
    "\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 5 # originally 10\n",
    "\n",
    "\n",
    "# GPU SETTINGS\n",
    "#TORCH_DEVICE = 'mps:0' # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "device = torch.device(\"mps\")\n",
    "GPU_MODE = 1 # set to 1 if want to run on gpu.\n",
    "\n",
    "\n",
    "# SETTINGS FOR DISPLAYING ON TENSORBOARD\n",
    "USE_TENSORBOARD = 0 #if you want to use tensorboard set this to 1.\n",
    "TENSORBOARD_SERVER = \"YOUR TENSORBOARD SERVER ADDRESS HERE\" # If you set.\n",
    "EXP_NAME = \"fine_tuning_experiment\" # if using tensorboard, enter name of experiment you want it to be displayed as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leery/Downloads/InDL.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Define transformations\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [transforms\u001b[39m.\u001b[39mToTensor(), transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m))], transforms\u001b[39m.\u001b[39mResize((img_size, img_size))) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# classes = (0, 1)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# # model_names = ['xception','densenet201','resnetv2_50', 'vgg16', 'darknet53', 'mobilenetv3_large_100', 'inception_resnet_v2', 'nasnetalarge', 'efficientnetv2_rw_m', 'convnext_xlarge_384_in22ft1k']\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model_names = []\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Define transformations###\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))], transforms.Resize((img_size, img_size))) \n",
    "\n",
    "# classes = (0, 1)\n",
    "# # model_names = ['xception','densenet201','resnetv2_50', 'vgg16', 'darknet53', 'mobilenetv3_large_100', 'inception_resnet_v2', 'nasnetalarge', 'efficientnetv2_rw_m', 'convnext_xlarge_384_in22ft1k']\n",
    "# model_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        path_img, label = self.data_info.iloc[item][1:3]\n",
    "        label = int(label)\n",
    "        path_img = os.path.join(self.data_dir, path_img)\n",
    "        image = Image.open(path_img).convert('RGB') # Gray scale is enough for logic interpretation\n",
    "        # 使用定义好的transforms，对数据进行处理\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    def get_img_info(self, data_dir):\n",
    "        path_dir = os.path.join(data_dir, 'label.csv')\n",
    "        return pd.read_csv(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to split training and val data in each folder\n",
    "\n",
    "def train_val_split(folder):\n",
    "    full_train = MyDataset(f\"/Users/leery/Documents/EllaNB240/indl-dataset-main/train/{dataset_folder}\", transform)\n",
    "    train_indices, val_indices = random_split(range(len(full_train)), [1200, 400]) \n",
    "    train_set = Subset(full_train, train_indices) \n",
    "    val_set = Subset(full_train, val_indices) # Store in dictionary \n",
    "    dsets = {'train': train_set, 'val': val_set}\n",
    "\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and val datasets\n",
    "separate_data = [] # Define empty list to store dicts for each folder\n",
    "\n",
    "for dataset_folder in [\"dataset01\", \"dataset02\", \"dataset03\", \"dataset04\", \"dataset05\"]:\n",
    "    separate_data.append(train_val_split(dataset_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine data into train and val datasets\n",
    "\n",
    "dsets = {}\n",
    "\n",
    "for i in range(len(separate_data)):\n",
    "#     dsets = {'train': separate_data[i-1]['train'], 'val': separate_data[i-1]['val']}\n",
    "#     new_items = {'train': separate_data[i]['train'], 'val': separate_data[i]['val']}\n",
    "#     print(new_items)\n",
    "#     dsets.update(new_items)\n",
    "\n",
    "    #dsets.setdefault('train', []).append(separate_data[i-1]['train'])\n",
    "    dsets.setdefault('train', []).extend(separate_data[i]['train'])\n",
    "\n",
    "    #dsets.setdefault('val', []).append(separate_data[i-1]['val'])\n",
    "    dsets.setdefault('val', []).extend(separate_data[i]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset sizes\n",
    "dset_sizes = {'train': len(dsets['train']), 'val': len(dsets['val'])}\n",
    "\n",
    "# Define DataLoaders\n",
    "dset_loaders = {\n",
    "    'train': DataLoader(dsets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=default_collate),\n",
    "    'val': DataLoader(dsets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=default_collate)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pertinent packages\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPU use\n",
    "use_gpu = GPU_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (TRYING SAME AS CIFAR-10 FOR NOW)\n",
    "# Source: https://www.kaggle.com/code/datascienceimcomming/cifar10-with-cnn-90-accuracy\n",
    "\n",
    "class SimpleResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, \n",
    "            out_channels=3, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=3, \n",
    "            out_channels=3, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        return self.relu2(out).to(device) + x\n",
    "\n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "        nn.BatchNorm2d(out_channels), \n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Model_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(\n",
    "            conv_block(128, 128), \n",
    "            conv_block(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(\n",
    "            conv_block(512, 512), \n",
    "            conv_block(512, 512)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(4), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train the model\n",
    "\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model.to(device)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    accuracies = {'train': [], 'val': []}\n",
    "    losses = {'train': [], 'val': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to eval mode for validation (no need to track gradients)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data, getting one batch of inputs (images) and labels each time.\n",
    "            for inputs, labels in dset_loaders[phase]:\n",
    "                #inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                #print(f\"Labels shape: {labels.shape}, Labels: {labels}, Inputs: {inputs}\")\n",
    "\n",
    "                if use_gpu:\n",
    "\n",
    "                    try:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    except Exception as e:\n",
    "                        print(\"ERROR! here are the inputs and labels before we print the full stack trace:\")\n",
    "                        print(inputs, labels)\n",
    "                        raise e\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Print a line every 10 batches so you have something to watch and don't feel like the program isn't running.\n",
    "                if counter%10==0:\n",
    "                    print(\"Reached batch iteration\", counter)\n",
    "\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                try:\n",
    "                    running_loss += loss.item()\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / float(dset_sizes[phase])\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            losses[phase].append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if USE_TENSORBOARD:\n",
    "                    foo.add_scalar_value('epoch_loss', epoch_loss,step=epoch)\n",
    "                    foo.add_scalar_value('epoch_acc', epoch_acc,step=epoch)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best accuracy =', best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "\n",
    "    return best_model, accuracies, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define function for modifying learning rate\n",
    "\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leery/miniconda3/envs/dalgi/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/leery/miniconda3/envs/dalgi/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.to(device)\n",
    "    model_ft.to(device)\n",
    "\n",
    "optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/4\n",
      "----------\n",
      "LR is set to 0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 128, 128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/leery/Downloads/InDL.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run the functions and save the best model in the function model_ft.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_ft, accuracies, losses \u001b[39m=\u001b[39m train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS)\n",
      "\u001b[1;32m/Users/leery/Downloads/InDL.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m counter\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Iterate over data, getting one batch of inputs (images) and labels each time.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m inputs, labels \u001b[39min\u001b[39;49;00m dset_loaders[phase]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m#inputs, labels = data\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39;49m inputs\u001b[39m.\u001b[39;49mto(device), labels\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leery/Downloads/InDL.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m#print(f\"Labels shape: {labels.shape}, Labels: {labels}, Inputs: {inputs}\")\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    157\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/dalgi/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [3, 128, 128] at entry 1"
     ]
    }
   ],
   "source": [
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft, accuracies, losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# batch 5 epoch 5\n",
    "# torch.save(model_ft.state_dict(), 'InDL_train_val.pt')\n",
    "\n",
    "# batch 15 epoch 10\n",
    "#torch.save(model_ft_2.state_dict(), 'InDL_train_val_2.pt')\n",
    "\n",
    "# batch 30 epoch 10\n",
    "torch.save(model_ft_4.state_dict(), 'InDL_train_val_4.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalgi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
