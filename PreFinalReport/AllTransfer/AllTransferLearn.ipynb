{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Subset \n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# CONFIG #\n",
    "# Adapted from Assignment 2 codebase\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "NUM_EPOCHS = 7\n",
    "\n",
    "\n",
    "# LEARNING RATE SETTINGS\n",
    "BASE_LR = 0.001\n",
    "DECAY_WEIGHT = 0.1 # factor by which the learning rate is reduced.\n",
    "EPOCH_DECAY = 30 # number of epochs after which the learning rate is decayed exponentially by DECAY_WEIGHT.\n",
    "\n",
    "\n",
    "# DATASET INFO\n",
    "NUM_CLASSES = 2 # set the number of classes in your dataset\n",
    "DATA_DIR = 'InDL' # to run with the sample dataset, just set to 'hymenoptera_data'\n",
    "\n",
    "\n",
    "# DATALOADER PROPERTIES\n",
    "BATCH_SIZE = 10 # originally 10\n",
    "\n",
    "\n",
    "# GPU SETTINGS\n",
    "#TORCH_DEVICE = 'mps:0' # Enter device ID of your gpu if you want to run on gpu. Otherwise neglect.\n",
    "device = torch.device(\"mps\")\n",
    "GPU_MODE = 1 # set to 1 if want to run on gpu.\n",
    "\n",
    "use_gpu = GPU_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InDL label dictionary\n",
    "import pickle \n",
    "\n",
    "with open('/Users/ellacho/Documents/NB240/indl-dataset-main/InDL_dict.p', 'rb') as f:\n",
    "    full_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Paste InDL custom dataset\n",
    "class InDL(Dataset):\n",
    "  def __init__(self, split = \"train\", transform=None, target_transform=None):\n",
    "\n",
    "    self.data_dir = \"/Users/ellacho/Documents/NB240/indl-dataset-main/data\"\n",
    "\n",
    "    self.transform = transform\n",
    "    self.split = split\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "    # Define class dictionary\n",
    "    self.class_dict = {i: f\"class[{i}]\" for i in range(0, 1)}\n",
    "\n",
    "    # Create empty list for storing filepath and image label\n",
    "    self.items = []\n",
    "\n",
    "    # Define logic to find training data and fill the dict\n",
    "\n",
    "    if self.split == 'train':\n",
    "       for folder in glob.glob(f\"{self.data_dir}/train/**/\"):\n",
    "          # Iterate through different .png files in each dataset folder\n",
    "          for file in os.listdir(folder):\n",
    "            if file.startswith(\".\"):  # skip .DS_Store and other hidden files\n",
    "              continue\n",
    "          \n",
    "            image_name = file\n",
    "            image_label = full_dict[image_name]\n",
    "            image_path = os.path.join(folder, image_name)\n",
    "\n",
    "            self.items.append((image_path, image_label))\n",
    "\n",
    "    elif self.split == 'val':\n",
    "       for folder in glob.glob(f\"{self.data_dir}/test/**/\"):\n",
    "          # Iterate through different .png files in each dataset folder\n",
    "          for file in os.listdir(folder):\n",
    "            if file.startswith(\".\"):  # skip .DS_Store and other hidden files\n",
    "              continue\n",
    "          \n",
    "            image_name = file\n",
    "            image_label = full_dict[image_name]\n",
    "            image_path = os.path.join(folder, image_name)\n",
    "\n",
    "            self.items.append((image_path, image_label))\n",
    "\n",
    "    print(f\"Loaded {len(self.items)} images for {self.split} dataset.\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.items)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    # Added additional code from generative AI (prompt: fix bug \"TypeError: Variable data has to be a tensor, but got list.\")\n",
    "    img_path, label = self.items[idx]\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    if self.transform:\n",
    "        image = self.transform(image)  # Ensure transform outputs a tensor\n",
    "    else:\n",
    "        image = transforms.ToTensor()(image)\n",
    "\n",
    "    label = torch.tensor(label, dtype=torch.long) # Ensure labels are integers, not lists or floats\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation\n",
    "# Adapted from Assignment 2 codebase\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 images for train dataset.\n",
      "Loaded 2000 images for val dataset.\n"
     ]
    }
   ],
   "source": [
    "# Define training and validation datasets\n",
    "dsets = {}\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    dsets[split] = InDL(split, data_transforms[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset sizes\n",
    "dset_sizes = {'train': len(dsets['train']), 'val': len(dsets['val'])}\n",
    "\n",
    "# Define DataLoaders\n",
    "dset_loaders = {\n",
    "    'train': DataLoader(dsets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=default_collate),\n",
    "    'val': DataLoader(dsets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=default_collate)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pertinent packages\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model.to(device)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    accuracies = {'train': [], 'val': []}\n",
    "    losses = {'train': [], 'val': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to eval mode for validation (no need to track gradients)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            counter = 0\n",
    "            # Iterate over data, getting one batch of inputs (images) and labels each time.\n",
    "            for data in dset_loaders[phase]:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Print a line every 10 batches so you have something to watch\n",
    "                if counter % 10 == 0:\n",
    "                    print(f\"Reached batch iteration {counter}\")\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Update running loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)  # Multiply by batch size for total loss\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            accuracies[phase].append(epoch_acc.item())  # Store accuracy for the phase\n",
    "            losses[phase].append(epoch_loss)  # Store loss for the phase\n",
    "\n",
    "            # Deep copy the model if it's the best so far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(f'New best accuracy = {best_acc:.4f}')\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        # lr_scheduler.step()\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    print('Returning the best model')\n",
    "\n",
    "    return best_model, accuracies, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY, DECAY_WEIGHT=0.1):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT ** (epoch // lr_decay_epoch))\n",
    "    \n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print(f\"LR is set to {lr}\")\n",
    "\n",
    "    # Update only the trainable parameters\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load ResNet50 model architecture\n",
    "model_ft = models.resnet50(pretrained=False)  # Start from scratch, don't load the ResNet50 weights\n",
    "\n",
    "# Load the previously trained ResNet18 weights (from your model)\n",
    "# shapenetsem = '/Users/ellacho/Documents/NB240/shapenetsem_4.pt'\n",
    "CIFAR10 = '/Users/ellacho/Documents/NB240/CIFAR10_train_val_3.pt'\n",
    "pretrained_weights = torch.load(CIFAR10)\n",
    "\n",
    "# Load the weights into the ResNet50 model\n",
    "# Note: You will need to adjust the keys since ResNet18 and ResNet50 are different architectures.\n",
    "# The general approach is to load the weights of common layers and skip the final layer (fc).\n",
    "model_ft.load_state_dict(pretrained_weights, strict=False)\n",
    "\n",
    "# Freeze all layers except the final classification layer\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Replace the final fully connected layer for your new task (e.g., new_num_classes = 5)\n",
    "num_features = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_features, 2)  # Replace with the new class count\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "if use_gpu:\n",
    "    model_ft.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "# Set the optimizer to only update the parameters of the new classification head (model.fc)\n",
    "optimizer_ft = optim.RMSprop(model_ft.fc.parameters(), lr=0.0001)  # Only optimize the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/6\n",
      "----------\n",
      "LR is set to 0.001\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6220 Acc: 0.6961\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6216 Acc: 0.7070\n",
      "New best accuracy = 0.7070\n",
      "LR is set to 0.001\n",
      "----------\n",
      "Epoch 1/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6168 Acc: 0.6982\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6136 Acc: 0.7010\n",
      "----------\n",
      "Epoch 2/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6156 Acc: 0.6966\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6155 Acc: 0.7070\n",
      "----------\n",
      "Epoch 3/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6141 Acc: 0.6966\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6232 Acc: 0.7070\n",
      "----------\n",
      "Epoch 4/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6128 Acc: 0.6973\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6166 Acc: 0.7070\n",
      "----------\n",
      "Epoch 5/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6144 Acc: 0.6982\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6323 Acc: 0.7005\n",
      "----------\n",
      "Epoch 6/6\n",
      "----------\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "Reached batch iteration 200\n",
      "Reached batch iteration 210\n",
      "Reached batch iteration 220\n",
      "Reached batch iteration 230\n",
      "Reached batch iteration 240\n",
      "Reached batch iteration 250\n",
      "Reached batch iteration 260\n",
      "Reached batch iteration 270\n",
      "Reached batch iteration 280\n",
      "Reached batch iteration 290\n",
      "Reached batch iteration 300\n",
      "Reached batch iteration 310\n",
      "Reached batch iteration 320\n",
      "Reached batch iteration 330\n",
      "Reached batch iteration 340\n",
      "Reached batch iteration 350\n",
      "Reached batch iteration 360\n",
      "Reached batch iteration 370\n",
      "Reached batch iteration 380\n",
      "Reached batch iteration 390\n",
      "Reached batch iteration 400\n",
      "Reached batch iteration 410\n",
      "Reached batch iteration 420\n",
      "Reached batch iteration 430\n",
      "Reached batch iteration 440\n",
      "Reached batch iteration 450\n",
      "Reached batch iteration 460\n",
      "Reached batch iteration 470\n",
      "Reached batch iteration 480\n",
      "Reached batch iteration 490\n",
      "Reached batch iteration 500\n",
      "Reached batch iteration 510\n",
      "Reached batch iteration 520\n",
      "Reached batch iteration 530\n",
      "Reached batch iteration 540\n",
      "Reached batch iteration 550\n",
      "Reached batch iteration 560\n",
      "Reached batch iteration 570\n",
      "Reached batch iteration 580\n",
      "Reached batch iteration 590\n",
      "Reached batch iteration 600\n",
      "Reached batch iteration 610\n",
      "Reached batch iteration 620\n",
      "Reached batch iteration 630\n",
      "Reached batch iteration 640\n",
      "Reached batch iteration 650\n",
      "Reached batch iteration 660\n",
      "Reached batch iteration 670\n",
      "Reached batch iteration 680\n",
      "Reached batch iteration 690\n",
      "Reached batch iteration 700\n",
      "Reached batch iteration 710\n",
      "Reached batch iteration 720\n",
      "Reached batch iteration 730\n",
      "Reached batch iteration 740\n",
      "Reached batch iteration 750\n",
      "Reached batch iteration 760\n",
      "Reached batch iteration 770\n",
      "Reached batch iteration 780\n",
      "Reached batch iteration 790\n",
      "train Loss: 0.6104 Acc: 0.6954\n",
      "Reached batch iteration 0\n",
      "Reached batch iteration 10\n",
      "Reached batch iteration 20\n",
      "Reached batch iteration 30\n",
      "Reached batch iteration 40\n",
      "Reached batch iteration 50\n",
      "Reached batch iteration 60\n",
      "Reached batch iteration 70\n",
      "Reached batch iteration 80\n",
      "Reached batch iteration 90\n",
      "Reached batch iteration 100\n",
      "Reached batch iteration 110\n",
      "Reached batch iteration 120\n",
      "Reached batch iteration 130\n",
      "Reached batch iteration 140\n",
      "Reached batch iteration 150\n",
      "Reached batch iteration 160\n",
      "Reached batch iteration 170\n",
      "Reached batch iteration 180\n",
      "Reached batch iteration 190\n",
      "val Loss: 0.6151 Acc: 0.7050\n",
      "Training complete in 9m 17s\n",
      "Best val Acc: 0.7070\n",
      "Returning the best model\n"
     ]
    }
   ],
   "source": [
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft, accuracies, losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export full_dict\n",
    "# import pickle\n",
    "\n",
    "# # # Save it as a pickle file\n",
    "# # with open('sns_accuracies.p', 'wb') as f:\n",
    "# #     pickle.dump(accuracies, f)\n",
    "\n",
    "# # with open('sns_losses.p', 'wb') as f:\n",
    "# #     pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 4, figsize = (12,4))\n",
    "fig.suptitle(\"All Transfer: CIFAR10\", fontsize=12) \n",
    "\n",
    "ax[0].plot(accuracies['train'])\n",
    "ax[0].set_title('Train Accuracies')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylim(0, 1)\n",
    "\n",
    "ax[1].plot(accuracies['val'])\n",
    "ax[1].set_title('Val Accuracies')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylim(0, 1)\n",
    "\n",
    "ax[2].plot(losses['train'])\n",
    "ax[2].set_title('Train Losses')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylim(0, 1)\n",
    "\n",
    "ax[3].plot(losses['val'])\n",
    "ax[3].set_title('Val Losses')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"AllTransfer_CIFAR10.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 10, 7 epochs\n",
    "torch.save(model_ft.state_dict(), 'CIFAR10_InDL_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for model evaluation\n",
    "# Source: ChatGPT; prompt: 'so after training, how do I perform eval'\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"Evaluation Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.7070\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_model_eval = evaluate_model(model_ft, dset_loaders['val'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NB240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
